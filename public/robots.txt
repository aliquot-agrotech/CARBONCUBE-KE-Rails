# See https://www.robotstxt.org/robotstxt.html for documentation on how to use the robots.txt file
# Robots.txt for Marketplace Platform
# This file controls how search engines crawl and index your site

# ==== ALLOW ALL SEARCH ENGINES ====
User-agent: *

# ==== BLOCK PRIVATE/SENSITIVE AREAS ====
# Admin areas - completely block from all crawlers
Disallow: /admin/
Disallow: /admin/*

# User authentication and account areas
Disallow: /auth/
Disallow: /auth/*

# Private seller areas
Disallow: /seller/
Disallow: /seller/*

# Private buyer areas  
Disallow: /buyer/
Disallow: /buyer/*

# Private rider areas
Disallow: /rider/
Disallow: /rider/*

# API endpoints (prevent indexing of raw JSON)
Disallow: /api/
Disallow: /api/*

# Payment and transaction pages
Disallow: /payments/
Disallow: /payments/*

# Search tracking and analytics
Disallow: /ad_searches
Disallow: /click_events

# WebSocket connections
Disallow: /cable
Disallow: /cable/*

# Development and health check endpoints
Disallow: /up
Disallow: /rails/

# ==== ALLOW PUBLIC AREAS ====
# Public product listings
Allow: /ads
Allow: /ads/*

# Public seller profiles (if you want them indexed)
Allow: /sellers/*/ads

# Public categories and navigation
Allow: /categories
Allow: /categories/*
Allow: /subcategories
Allow: /subcategories/*

# Public information pages
Allow: /banners
Allow: /counties
Allow: /age_groups
Allow: /incomes
Allow: /sectors
Allow: /educations
Allow: /employments
Allow: /tiers
Allow: /document_types

# ==== CRAWL DELAY ====
# Prevent aggressive crawling that could slow down your site
Crawl-delay: 1

# ==== SITEMAP LOCATION ====
# Point crawlers to your sitemap 
Sitemap: https://carboncube-ke.com/sitemap.xml

# ==== SPECIFIC SEARCH ENGINE RULES ====

# Google Bot - allow more frequent crawling
User-agent: Googlebot
Crawl-delay: 1
Allow: /ads
Allow: /ads/*
Allow: /sellers/*/ads
Disallow: /admin/
Disallow: /seller/
Disallow: /buyer/
Disallow: /rider/
Disallow: /auth/
Disallow: /api/

# Bing Bot
User-agent: Bingbot
Crawl-delay: 2
Allow: /ads
Allow: /ads/*
Disallow: /admin/
Disallow: /seller/
Disallow: /buyer/
Disallow: /rider/
Disallow: /auth/
Disallow: /api/

# Block aggressive crawlers that might harm performance
User-agent: AhrefsBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: MJ12bot
Disallow: /

# ==== NOTES ====
# 1. Place this file at: public/robots.txt
# 2. Test at: https://carboncube-ke.com/robots.txt
# 3. Submit to Google Search Console
# 4. Create and submit sitemap.xml
# 5. Monitor crawl errors in search console